{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad81037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Author: Cordell Stonecipher\n",
    "Filename: cnn_stock_module.ipynb\n",
    "Description:\n",
    "Provides functions to build, train, tune, predict, and interpret a 1D CNN for stock price\n",
    "Buy/Sell/Hold classification. This module\n",
    "accepts NumPy arrays for training, validation, and test sets.\n",
    "\n",
    "Functions:\n",
    "  - build_cnn_model(input_shape, hp=None): returns compiled Keras model or HyperModel\n",
    "  - tune_hyperparameters(X_train, y_train, X_val, y_val, max_trials=10): returns best model and HP\n",
    "  - train_model(model, X_train, y_train, X_val, y_val, epochs=30, batch_size=64)\n",
    "      Trains and returns history\n",
    "  - evaluate_model(model, X, y, class_names): prints metrics\n",
    "  - predict_sample(model, sample, class_names, weights=None): returns predicted class and probabilities\n",
    "  - create_explainer(X_train, feature_names, class_names): returns LIME explainer\n",
    "  - explain_instance(model, explainer, sample_flat, num_features, class_index)\n",
    "      returns LIME explanation list\n",
    "\n",
    "Dependencies:\n",
    "  pip install tensorflow keras-tuner lime numpy\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from lime import lime_tabular\n",
    "\n",
    "# WINDOW must match the temporal window length for each input sample\n",
    "WINDOW = None  # set before calling explain_instance or predict_sample\n",
    "\n",
    "\n",
    "def build_cnn_model(input_shape, hp=None):\n",
    "    \"\"\"\n",
    "    Builds a 1D CNN for time-series classification.\n",
    "    Convolutional layers act as learnable sliding-window feature extractors:\n",
    "      Conv1D: applies filters (f) of width (k) across the time axis, computing dot products.\n",
    "      MaxPooling1D: downsamples by taking the maximum over non-overlapping windows, reducing temporal dimension.\n",
    "    Flatten: converts the 3D tensor [batch, time, features] to a 2D vector for dense layers.\n",
    "    Dense: fully connected layers apply weight matrices W and biases b, computing x -> ReLU(Wx+b).\n",
    "    Dropout: randomly zeroes a fraction d of inputs to prevent overfitting (approximate model averaging).\n",
    "    Output layer: softmax activation computes exp(z_i)/sum_j exp(z_j), giving class probabilities.\n",
    "    Loss: categorical_crossentropy = -sum(y_true * log(y_pred)).\n",
    "    Optimizer: Adam uses adaptive learning rates with momentum (estimates of first/second moments).\n",
    "\n",
    "    Hyperparameters tune:\n",
    "      - number of conv blocks\n",
    "      - filters per conv layer\n",
    "      - kernel sizes\n",
    "      - dense units\n",
    "      - dropout rate\n",
    "      - learning rate\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = inp\n",
    "\n",
    "    # Build conv+pool blocks\n",
    "    num_blocks = hp.Int('conv_blocks', 1, 3, default=2) if hp else 2\n",
    "    for i in range(num_blocks):\n",
    "        filters = hp.Choice(f'filters_{i}', [32,64,128], default=64) if hp else 64\n",
    "        kernel_size = hp.Choice(f'kernel_{i}', [3,5], default=3) if hp else 3\n",
    "        # Convolution: y[t, f] = sum_{u=0..k-1}( x[t+u, :] * W[:, u, f] ) + b[f]\n",
    "        x = layers.Conv1D(filters, kernel_size, activation='relu', padding='same')(x)\n",
    "        # Pooling: y[t', f] = max( x[2*t':2*t'+2, f] ) -- halves time dimension\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Flatten to vector: shape -> [batch, filters * (time/2^blocks)]\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Dense layer: x -> ReLU(Wx + b)\n",
    "    dense_units = hp.Int('dense_units', 64, 256, step=64, default=128) if hp else 128\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    # Dropout: zero-out fraction to regularize\n",
    "    dropout_rate = hp.Float('dropout', 0.1, 0.5, step=0.1, default=0.2) if hp else 0.2\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer: softmax for 3 classes\n",
    "    output = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=output)\n",
    "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4], default=1e-4) if hp else 1e-4\n",
    "    # Adam optimizer: alpha=lr, uses moving averages of gradients\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train, X_val, y_val, max_trials=10):\n",
    "    \"\"\"\n",
    "    Bayesian optimization over hyperparameters to maximize validation accuracy.\n",
    "    Returns the best model and hyperparameter configuration.\n",
    "    \"\"\"\n",
    "    def model_builder(hp):\n",
    "        return build_cnn_model(X_train.shape[1:], hp)\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        model_builder,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=max_trials,\n",
    "        directory='cnn_tuner',\n",
    "        project_name='cnn_stock'\n",
    "    )\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_hps   = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_model, best_hps\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=30, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train the CNN with early stopping to avoid overfitting.\n",
    "    EarlyStopping monitors validation loss, stops training if no improvement.\n",
    "    \"\"\"\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, class_names=['Buy','Sell','Hold']):\n",
    "    \"\"\"\n",
    "    Compute predictions, then accuracy and classification report.\n",
    "    \"\"\"\n",
    "    preds = np.argmax(model.predict(X), axis=1)\n",
    "    true  = np.argmax(y, axis=1) if y.ndim>1 else y\n",
    "    acc   = accuracy_score(true, preds)\n",
    "    print(f\"Accuracy: {acc:.2%}\")\n",
    "    print(classification_report(true, preds, target_names=class_names))\n",
    "\n",
    "\n",
    "def predict_sample(model, sample, class_names=['Buy','Sell','Hold'], weights=None):\n",
    "    \"\"\"\n",
    "    Predict on one sample: apply softmax probabilities,\n",
    "    optionally reweight probabilities, then renormalize.\n",
    "    \"\"\"\n",
    "    probs = model.predict(sample.reshape(1, *sample.shape))[0]\n",
    "    if weights is not None:\n",
    "        w = np.array(weights)\n",
    "        probs = probs * w\n",
    "        probs = probs / probs.sum()\n",
    "    cls = np.argmax(probs)\n",
    "    return class_names[cls], probs\n",
    "\n",
    "\n",
    "def create_explainer(X_train_flat, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Initialize LIME explainer using training distribution.\n",
    "    Each feature is treated as independent for tabular LIME.\n",
    "    \"\"\"\n",
    "    return lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train_flat,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification'\n",
    "    )\n",
    "\n",
    "\n",
    "def explain_instance(model, explainer, sample_flat, num_features=10, class_index=0):\n",
    "    \"\"\"\n",
    "    Generate local linear approximation around sample to explain model's decision.\n",
    "    Solves a weighted linear regression locally: minimize ||f(x') - L(x')|| + Î»||L||.\n",
    "    \"\"\"\n",
    "    if WINDOW is None:\n",
    "        raise ValueError(\"Set WINDOW to your time-step length before calling explain_instance.\")\n",
    "    exp = explainer.explain_instance(\n",
    "        data_row=sample_flat,\n",
    "        predict_fn=lambda x: model.predict(x.reshape(-1, WINDOW, sample_flat.size//WINDOW)),\n",
    "        num_features=num_features,\n",
    "        labels=(class_index,)\n",
    "    )\n",
    "    return exp.as_list(label=class_index)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
